package repository

import (
	"encoding/binary"
	"encoding/hex"
	"encoding/json"
	"strconv"
	"strings"
	"sync"

	"fmt"

	"wfts/internal/model"
	"wfts/pkg/logger"
	"github.com/dgraph-io/badger/v3"
)

type IndexRepository struct {
	DB 				*badger.DB
	log 			*logger.Logger
	wg 				*sync.WaitGroup
	mu 				*sync.Mutex
	nGramIndexer	*wordChunkData
	shingleIndexer	*shingleChunkData
	chunkSize 		int
}

func NewIndexRepository(path string, logger *logger.Logger, chunkSize int) (*IndexRepository, error) {
	db, err := badger.Open(badger.DefaultOptions(path).WithLoggingLevel(badger.WARNING))
	db.CacheMaxCost(badger.BlockCache, 64 << 20)
	if err != nil {
		return nil, err
	}
	ir := &IndexRepository{
		DB: db,
		log: logger,
		wg: new(sync.WaitGroup),
		mu: new(sync.Mutex),
		nGramIndexer: &wordChunkData{buffer: make(map[string][]string), counts: make(map[string]int)},
		shingleIndexer: &shingleChunkData{buffer: make(map[[4]uint64][][128]uint64), counts: make(map[[4]uint64]int)},
		chunkSize: chunkSize,
	}
	return ir, ir.UpdateChunkingCounts() // сомнительно потому что нам не нужно это прокидывать если мы не будем индексировать
}

func (ir *IndexRepository) LoadVisitedUrls(visitedURLs *sync.Map) error {
    opts := badger.DefaultIteratorOptions
    opts.Prefix = []byte("visited:")

    return ir.DB.View(func(txn *badger.Txn) error {
        it := txn.NewIterator(opts)
        defer it.Close()
        for it.Rewind(); it.Valid(); it.Next() {
            item := it.Item()
            key := string(item.Key())
            url := strings.TrimPrefix(key, "visited:")
			depth, err := item.ValueCopy(nil)
			if err != nil {
				return err
			}
			d, err := strconv.Atoi(string(depth))
			if err != nil {
				return err
			}
            visitedURLs.Store(url, d)
        }
        return nil
    })
}

func (ir *IndexRepository) SaveVisitedUrls(visitedURLs *sync.Map) error {
	visitedURLs.Range(func(key, value any) bool {
		if url, ok := key.(string); ok {
			ir.DB.Update(func(txn *badger.Txn) error {
				return txn.Set([]byte("visited:" + url), fmt.Append(nil, value.(int)))
			})
		}
		return true
	})
	return nil
}

func (ir *IndexRepository) IndexDocumentWords(docID [32]byte, sequence map[string]int, pos map[string][]model.Position) error {
	ir.mu.Lock()
	defer ir.mu.Unlock()

	type wordEntry struct {
		word string
		freq int
	}
	
	entries := make([]wordEntry, 0, len(sequence))
	for w, f := range sequence {
		entries = append(entries, wordEntry{word: w, freq: f})
	}

	const iterSize = 500
	for i := 0; i < len(entries); i += iterSize {
		chunk := entries[i: min(len(entries), i + iterSize)]

		if err := ir.DB.Update(func(txn *badger.Txn) error {
			for _, entry := range chunk {
				key := fmt.Appendf(nil, WordDocumentKeyFormat, entry.word, docID)
				positions := pos[entry.word]
				if len(positions) > 500 {
					positions = positions[:500] // более 500 вхождений одного слова в один документ....
				}

				wcp := model.WordCountAndPositions{
					Count:     entry.freq,
					Positions: positions,
				}
				val, err := json.Marshal(wcp)
				if err != nil {
					return err
				}
				if len(val) > 1024 * 1024 { // нужен ли нам текстовый токен более 1 мб? я думаю нет, я правда не сильно верю что это условие вообще хоть раз отработает
					continue
				}
				if err := txn.Set(key, val); err != nil {
					return err
				}
			}
			return nil
		}); err != nil {
			return fmt.Errorf("failed to update chunk %d: %w", i, err)
		}
	}

	return nil
}

func (ir *IndexRepository) GetDocumentsByWord(word string) (map[[32]byte]model.WordCountAndPositions, error) {
	revertWordIndex := make(map[[32]byte]model.WordCountAndPositions)
	wprefix := fmt.Appendf(nil, "ri:%s_", word)
	return revertWordIndex, ir.DB.View(func(txn *badger.Txn) error {
		it := txn.NewIterator(badger.DefaultIteratorOptions)
		defer it.Close()
		for it.Seek(wprefix); it.ValidForPrefix(wprefix); it.Next() {
			item := it.Item()
			keyPart := item.Key()[len(wprefix):]

			decoded, err := hex.DecodeString(string(keyPart))
			if err != nil {
				return err
			}
			id := [32]byte{}
			copy(id[:], decoded)

			val, err := item.ValueCopy(nil)
			if err != nil {
				return err
			}
			positions := model.WordCountAndPositions{}
			if err := json.Unmarshal(val, &positions); err != nil {
				return err
			}

			revertWordIndex[id] = positions
		}
		
		return nil
	})
}

const biK = "big:%d:%d"

func (ir *IndexRepository) UpdateBiFreq(biS map[[2]uint64]int) error {
	for lr, freq := range biS {
		if err := ir.DB.Update(func(txn *badger.Txn) error {
			key := fmt.Appendf(nil, biK, lr[0], lr[1])
			item, err := txn.Get(key)
			if err != nil && err != badger.ErrKeyNotFound {
				return err
			}
			if err != badger.ErrKeyNotFound {
				val, err := item.ValueCopy(nil)
				if err != nil {
					return err
				}
				freq += decCount(val)
			}
			return txn.Set(key, encCount(freq))
		}); err != nil {
			return err
		}
	}
	return nil
}

func (ir *IndexRepository) GetFreq(l, r uint64) (int, error) {
	freq := 0
	return freq, ir.DB.View(func(txn *badger.Txn) error {
		it, err := txn.Get(fmt.Appendf(nil, biK, l, r))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return nil
			}
			return err
		}
		val, err := it.ValueCopy(nil)
		if err != nil {
			return err
		}
		freq = decCount(val)
		return nil
	})
}

func encCount(n int) []byte {
	buf := make([]byte, 4)
	binary.BigEndian.PutUint32(buf, uint32(n))
	return buf
}

func decCount(c []byte) int {
	return int(binary.BigEndian.Uint32(c))
}